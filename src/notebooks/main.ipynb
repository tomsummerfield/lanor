{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym.wrappers import FlattenObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_gym.kube_env import KubeGymEnv\n",
    "env = KubeGymEnv()\n",
    "env.reset()\n",
    "env = FlattenObservation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "        self.logprobs = []\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "        return np.array(self.states),\\\n",
    "               np.array(self.actions),\\\n",
    "               np.array(self.rewards),\\\n",
    "               np.array(self.dones),\\\n",
    "               np.array(self.values),\\\n",
    "               np.array(self.logprobs),\\\n",
    "               batches\n",
    "\n",
    "    def store_memory(self, state, action, reward, done, probs, vals):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)       \n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)        \n",
    "        self.values.append(vals)\n",
    "        self.logprobs.append(probs)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, n_actions):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dims, hidden_dims, dtype=torch.float32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, hidden_dims, dtype=torch.float32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, n_actions, dtype=torch.float32),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Return the probabilities directly instead of creating a distribution\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dims, hidden_dims, dtype=torch.float32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, hidden_dims, dtype=torch.float32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, 1, dtype=torch.float32)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        value = self.layers(x)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, \n",
    "                n_actions, \n",
    "                input_dims, \n",
    "                hidden_dims,\n",
    "                gamma=0.99, \n",
    "                alpha=0.003, \n",
    "                policy_clip=0.2, \n",
    "                batch_size=32, \n",
    "                n_epochs=10,\n",
    "                gae_lambda=0.95,  # Add this parameter\n",
    "                chkpt_dir='models/'\n",
    "    ):\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda  # Store the parameter\n",
    "        self.actor = ActorNetwork(input_dims, hidden_dims, n_actions)\n",
    "        self.critic = CriticNetwork(input_dims, hidden_dims)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=alpha)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=alpha)\n",
    "        self.chkpt_dir = chkpt_dir\n",
    "\n",
    "    def store_transition(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save(self.actor.state_dict(), self.chkpt_dir + 'actor')\n",
    "        self.critic.save(self.critic.state_dict(), self.chkpt_dir + 'critic')\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor = self.actor.load_state_dict(torch.load(self.chkpt_dir + 'actor'))\n",
    "        self.critic = self.critic.load_state_dict(torch.load(self.chkpt_dir + 'critic'))\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = torch.tensor([observation], dtype=torch.float32)\n",
    "\n",
    "        probs = self.actor(state)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Detach tensors and convert action to integer\n",
    "        action = int(action.detach().numpy()[0])  # Convert to integer\n",
    "        value = value.detach().numpy()[0]\n",
    "        log_prob = log_prob.detach().numpy()[0]\n",
    "\n",
    "        return action, log_prob, value\n",
    "    \n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "                reward_arr, dones_arr, batches = \\\n",
    "                self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            # Calculate advantages\n",
    "            for t in range(len(reward_arr)):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)):\n",
    "                    # Calculate next value\n",
    "                    if k < len(reward_arr) - 1:\n",
    "                        next_val = values[k + 1]\n",
    "                    else:\n",
    "                        next_val = 0  # Terminal state has value 0\n",
    "                    \n",
    "                    delta = reward_arr[k] + self.gamma * next_val * (1 - int(dones_arr[k])) - values[k]\n",
    "                    a_t += discount * delta\n",
    "                    discount *= self.gamma * self.gae_lambda\n",
    "                advantage[t] = float(a_t)  # Explicitly convert to float\n",
    "\n",
    "            advantage = torch.tensor(advantage, dtype=torch.float32)\n",
    "            values = torch.tensor(values, dtype=torch.float32)\n",
    "\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float32)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch], dtype=torch.float32)\n",
    "                actions = torch.tensor(action_arr[batch], dtype=torch.float32)\n",
    "\n",
    "                # Actor loss\n",
    "                probs = self.actor(states)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                \n",
    "                prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip, 1+self.policy_clip)\n",
    "                weighted_clipped_probs = clipped_probs * advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                # Critic loss \n",
    "                critic_value = self.critic(states).squeeze()\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                criterion = nn.MSELoss()  # Create MSELoss instance\n",
    "                critic_loss = criterion(critic_value, returns)  # Use the instance\n",
    "\n",
    "                # Take gradient steps\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch  # Using torch instead of tensorflow since project uses PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "def train_agent(batch_size, n_games, n_epochs, alpha, agent, N, n_steps):\n",
    "    score_history = []\n",
    "    learn_iters = 0  # Move this inside the function\n",
    "    best_score = env.reward_range[0]  # Move this inside too\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        if isinstance(observation, tuple):\n",
    "            observation = observation[0]\n",
    "\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(np.expand_dims(observation, axis=0))\n",
    "            print(f\"action: {action}, prob: {prob}, val: {val}\")\n",
    "\n",
    "            observation, reward, done, _,  info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action,\n",
    "                                    prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        \n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "        \n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "        env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 1, prob: [-0.4483865], val: [[-0.22839773]]\n",
      "action: 0, prob: [-0.5292991], val: [[-0.15399393]]\n",
      "action: 0, prob: [-0.9890362], val: [[-0.22839773]]\n",
      "action: 0, prob: [-0.94832015], val: [[-0.30311644]]\n",
      "action: 1, prob: [-0.74031675], val: [[-0.13404432]]\n",
      "action: 1, prob: [-0.42432192], val: [[-0.22839773]]\n",
      "action: 0, prob: [-0.9653784], val: [[-0.30266893]]\n",
      "action: 1, prob: [-1.7424992], val: [[-0.26524082]]\n",
      "action: 0, prob: [-1.2510605], val: [[-0.22839773]]\n",
      "action: 0, prob: [-1.1628823], val: [[-0.22839773]]\n",
      "action: 0, prob: [-0.56550276], val: [[-0.22106117]]\n",
      "action: 1, prob: [-0.49120742], val: [[-0.17101063]]\n",
      "action: 1, prob: [-0.39791977], val: [[-0.22839773]]\n",
      "action: 1, prob: [-0.00229138], val: [[0.1409075]]\n",
      "action: 0, prob: [-0.5137265], val: [[-0.23170257]]\n",
      "action: 0, prob: [-0.99991786], val: [[-0.2660547]]\n",
      "action: 1, prob: [-0.69412637], val: [[-0.25616357]]\n",
      "action: 1, prob: [-0.5077441], val: [[-0.22839773]]\n",
      "action: 1, prob: [-0.51320404], val: [[-0.22839773]]\n",
      "action: 0, prob: [-0.6053241], val: [[-0.46909377]]\n",
      "action: 0, prob: [-1.0206362], val: [[-0.62692547]]\n",
      "action: 2, prob: [-1.520464], val: [[-0.60716337]]\n",
      "action: 1, prob: [-0.55887055], val: [[-0.36076853]]\n",
      "action: 1, prob: [-0.5774316], val: [[-0.38022292]]\n",
      "action: 1, prob: [-0.44863665], val: [[-0.23987238]]\n",
      "action: 0, prob: [-0.825021], val: [[-0.45053095]]\n",
      "action: 0, prob: [-0.998375], val: [[-0.34633714]]\n",
      "action: 1, prob: [-0.9124023], val: [[-0.59471095]]\n",
      "action: 1, prob: [-0.607372], val: [[-0.4747346]]\n",
      "action: 1, prob: [-1.1532474], val: [[-0.64425224]]\n",
      "action: 0, prob: [-0.71282464], val: [[-0.35949963]]\n",
      "action: 1, prob: [-1.7267859], val: [[-0.36695024]]\n",
      "action: 0, prob: [-0.9975443], val: [[-0.3113013]]\n",
      "action: 1, prob: [-0.00482643], val: [[-0.23987238]]\n",
      "action: 2, prob: [-3.029132], val: [[-0.32641527]]\n",
      "action: 2, prob: [-3.1031046], val: [[-0.23987238]]\n",
      "action: 0, prob: [-0.73510695], val: [[-0.32844937]]\n",
      "action: 1, prob: [-0.4693272], val: [[-0.23987238]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\summe\\AppData\\Local\\Temp\\ipykernel_535164\\2258148201.py:48: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  action = int(action.detach().numpy()[0])  # Convert to integer\n",
      "C:\\Users\\summe\\AppData\\Local\\Temp\\ipykernel_535164\\2258148201.py:77: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  advantage[t] = float(a_t)  # Explicitly convert to float\n",
      "c:\\Users\\summe\\OneDrive\\Documents\\New-Projects\\lanor\\env\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([4, 1, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 19 is out of bounds for axis 0 with size 19",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(n_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \n\u001b[0;32m      8\u001b[0m               batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m      9\u001b[0m               alpha\u001b[38;5;241m=\u001b[39malpha, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m               hidden_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     13\u001b[0m               gae_lambda\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n\u001b[0;32m     15\u001b[0m n_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[132], line 28\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(batch_size, n_games, n_epochs, alpha, agent, N, n_steps)\u001b[0m\n\u001b[0;32m     25\u001b[0m agent\u001b[38;5;241m.\u001b[39mstore_transition(observation, action,\n\u001b[0;32m     26\u001b[0m                         prob, val, reward, done)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_steps \u001b[38;5;241m%\u001b[39m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 28\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     learn_iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     30\u001b[0m observation \u001b[38;5;241m=\u001b[39m observation\n",
      "Cell \u001b[1;32mIn[131], line 70\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t, \u001b[38;5;28mlen\u001b[39m(reward_arr)):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Calculate next value\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(reward_arr) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 70\u001b[0m         next_val \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m         next_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Terminal state has value 0\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 19 is out of bounds for axis 0 with size 19"
     ]
    }
   ],
   "source": [
    "# Then call it with:\n",
    "N = 20  # Steps before learning\n",
    "batch_size = 5\n",
    "n_games = 300\n",
    "n_epochs = 10\n",
    "alpha = 0.0003\n",
    "agent = Agent(n_actions=3, \n",
    "              batch_size=batch_size,\n",
    "              alpha=alpha, \n",
    "              n_epochs=n_epochs,\n",
    "              input_dims=env.observation_space.shape[0], \n",
    "              hidden_dims=10,\n",
    "              gae_lambda=0.95)\n",
    "\n",
    "n_steps = 0\n",
    "train_agent(batch_size, n_games, n_epochs, alpha, agent, N, n_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
